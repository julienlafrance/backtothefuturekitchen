#!/usr/bin/env python3
"""
Analyse compl√®te de toutes les tables DuckDB avec YData SDK
Bas√© sur ydata_sdk_documentation_complete.md
"""

import os
import sys
import duckdb
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Import YData SDK components
from ydata_profiling import ProfileReport

def load_env_config():
    """Charger la configuration depuis le fichier .env"""
    if os.path.exists('.env'):
        with open('.env', 'r') as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    try:
                        key, value = line.strip().split('=', 1)
                        os.environ[key] = value
                    except ValueError:
                        continue
    
    # V√©rifier si le token est disponible
    token = os.environ.get('YDATA_LICENSE_KEY')
    if not token:
        print("‚ö†Ô∏è YDATA_LICENSE_KEY non trouv√© dans .env")
    else:
        print(f"‚úÖ YDATA_LICENSE_KEY configur√©: {token[:10]}...")
    
    return token

def connect_to_duckdb():
    """Connexion √† la base DuckDB de production"""
    db_path = '/home/dataia25/mangetamain/10_prod/data/mangetamain.duckdb'
    
    if not os.path.exists(db_path):
        raise FileNotFoundError(f"Base DuckDB non trouv√©e: {db_path}")
    
    conn = duckdb.connect(db_path)
    print(f"‚úÖ Connexion DuckDB: {db_path}")
    return conn

def get_table_info(conn):
    """R√©cup√©rer les informations sur toutes les tables"""
    tables = conn.execute('SHOW TABLES').fetchall()
    table_info = {}
    
    print("\nüîç Tables disponibles:")
    for table in tables:
        table_name = table[0]
        count = conn.execute(f'SELECT COUNT(*) FROM {table_name}').fetchone()[0]
        
        # R√©cup√©rer le sch√©ma
        schema = conn.execute(f'DESCRIBE {table_name}').fetchall()
        columns = [col[0] for col in schema]
        
        table_info[table_name] = {
            'count': count,
            'columns': columns,
            'schema': schema
        }
        
        print(f"  üìä {table_name}: {count:,} lignes, {len(columns)} colonnes")
    
    return table_info

def detect_temporal_columns(df, table_name):
    """D√©tecter les colonnes temporelles dans le DataFrame"""
    temporal_columns = []
    
    for col in df.columns:
        # V√©rifier si c'est une colonne datetime
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            temporal_columns.append(col)
            continue
            
        # V√©rifier si c'est une colonne qui pourrait √™tre temporelle
        if col.lower() in ['date', 'timestamp', 'time', 'submitted', 'created', 'modified']:
            # Essayer de parser comme date
            try:
                sample = df[col].dropna().head(100)
                if len(sample) > 0:
                    parsed = pd.to_datetime(sample, errors='coerce')
                    if parsed.notna().sum() > len(sample) * 0.8:  # 80% de parsing r√©ussi
                        temporal_columns.append(col)
            except:
                continue
    
    if temporal_columns:
        print(f"  ‚è∞ Colonnes temporelles d√©tect√©es dans {table_name}: {temporal_columns}")
    
    return temporal_columns

def create_profile_report(df, table_name, temporal_columns=None):
    """Cr√©er un rapport de profiling YData avec configuration optimis√©e"""
    
    print(f"üìä G√©n√©ration du profil pour {table_name}...")
    
    # Configuration de base
    config = {
        'title': f'Analyse YData - {table_name}',
        'dataset': {
            'description': f'Profil complet de la table {table_name}',
            'creator': 'YData SDK Analysis'
        },
        'variables': {
            'descriptions': {},
            'num': {
                'low_categorical_threshold': 10
            }
        },
        'correlations': {
            'pearson': {'calculate': True},
            'spearman': {'calculate': True},
            'kendall': {'calculate': False}  # Trop lent pour gros datasets
        },
        'missing_diagrams': {
            'bar': True,
            'matrix': True,
            'heatmap': True
        },
        'interactions': {
            'targets': []
        }
    }
    
    # Configuration sp√©ciale pour time series si colonnes temporelles d√©tect√©es
    if temporal_columns:
        config['tsmode'] = True
        if len(temporal_columns) > 0:
            config['sortby'] = temporal_columns[0]  # Utiliser la premi√®re colonne temporelle
        print(f"  üïí Mode time series activ√© pour {temporal_columns}")
    
    # Adapter la configuration selon la taille du dataset
    if len(df) > 100000:
        # Pour les gros datasets, simplifier l'analyse
        config['correlations']['spearman']['calculate'] = False
        config['interactions']['calculate'] = False
        print(f"  ‚ö° Configuration optimis√©e pour dataset large ({len(df):,} lignes)")
    
    try:
        # Cr√©er le rapport
        profile = ProfileReport(
            df, 
            **config
        )
        
        # Sauvegarder
        output_path = f"ydata_analysis/{table_name}_complete_profile.html"
        profile.to_file(output_path)
        
        print(f"  ‚úÖ Rapport sauvegard√©: {output_path}")
        return profile
        
    except Exception as e:
        print(f"  ‚ùå Erreur lors du profiling de {table_name}: {e}")
        return None

def export_table_to_csv(conn, table_name):
    """Exporter la table vers CSV pour analyse compl√©mentaire"""
    try:
        # Pour les gros datasets, limiter l'export
        count = conn.execute(f'SELECT COUNT(*) FROM {table_name}').fetchone()[0]
        
        if count > 500000:
            # √âchantillon pour tr√®s gros datasets
            df = conn.execute(f'SELECT * FROM {table_name} USING SAMPLE 100000').df()
            csv_path = f"ydata_analysis/{table_name}_sample.csv"
            print(f"  üìÅ Export √©chantillon (100k lignes sur {count:,}): {csv_path}")
        else:
            # Export complet pour datasets raisonnables
            df = conn.execute(f'SELECT * FROM {table_name}').df()
            csv_path = f"ydata_analysis/{table_name}.csv"
            print(f"  üìÅ Export complet: {csv_path}")
        
        df.to_csv(csv_path, index=False)
        return df, csv_path
        
    except Exception as e:
        print(f"  ‚ùå Erreur export CSV {table_name}: {e}")
        return None, None

def analyze_all_tables():
    """Analyse compl√®te de toutes les tables"""
    
    print("üöÄ D√©marrage de l'analyse compl√®te YData SDK")
    print("=" * 60)
    
    # Configuration
    load_env_config()
    
    # Connexion base
    conn = connect_to_duckdb()
    
    # Information tables
    table_info = get_table_info(conn)
    
    print(f"\nüìã Analyse de {len(table_info)} tables:")
    print("=" * 60)
    
    results = {}
    
    for table_name in table_info.keys():
        print(f"\nüîÑ Traitement de {table_name}...")
        
        try:
            # Export vers CSV
            df, csv_path = export_table_to_csv(conn, table_name)
            
            if df is not None:
                # D√©tection colonnes temporelles
                temporal_columns = detect_temporal_columns(df, table_name)
                
                # G√©n√©ration profil YData
                profile = create_profile_report(df, table_name, temporal_columns)
                
                results[table_name] = {
                    'status': 'success',
                    'rows': len(df),
                    'columns': len(df.columns),
                    'temporal_columns': temporal_columns,
                    'csv_path': csv_path,
                    'profile': profile is not None
                }
                
                print(f"  ‚úÖ {table_name} analys√© avec succ√®s")
            else:
                results[table_name] = {
                    'status': 'error',
                    'error': 'Export CSV √©chou√©'
                }
                
        except Exception as e:
            print(f"  ‚ùå Erreur {table_name}: {e}")
            results[table_name] = {
                'status': 'error',
                'error': str(e)
            }
    
    # R√©sum√© final
    print("\n" + "=" * 60)
    print("üìä R√âSUM√â DE L'ANALYSE")
    print("=" * 60)
    
    total_rows = 0
    successful_tables = 0
    temporal_tables = []
    
    for table_name, result in results.items():
        if result['status'] == 'success':
            successful_tables += 1
            total_rows += result['rows']
            
            if result['temporal_columns']:
                temporal_tables.append({
                    'table': table_name,
                    'temporal_columns': result['temporal_columns']
                })
            
            print(f"‚úÖ {table_name}: {result['rows']:,} lignes, {result['columns']} colonnes")
        else:
            print(f"‚ùå {table_name}: {result['error']}")
    
    print(f"\nüìà Statistiques globales:")
    print(f"  - Tables analys√©es: {successful_tables}/{len(table_info)}")
    print(f"  - Total lignes: {total_rows:,}")
    print(f"  - Tables avec donn√©es temporelles: {len(temporal_tables)}")
    
    if temporal_tables:
        print(f"\nüïí Tables temporelles identifi√©es:")
        for item in temporal_tables:
            print(f"  - {item['table']}: {item['temporal_columns']}")
    
    conn.close()
    print(f"\nüéØ Analyse termin√©e ! Rapports disponibles dans: ydata_analysis/")
    
    return results

if __name__ == "__main__":
    results = analyze_all_tables()
