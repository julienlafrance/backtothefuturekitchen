#!/usr/bin/env python3
"""
Test pytest pour v√©rifier toutes les requ√™tes SQL du projet

Ce script:
1. Scanne automatiquement tous les fichiers Python et notebooks
2. Extrait toutes les requ√™tes SQL (DuckDB, S3, tables locales)
3. Teste leur validit√© et ex√©cution

Usage:
    pytest test_sql_queries.py -v
    pytest test_sql_queries.py -v -s  # avec output d√©taill√©
"""

import pytest
import duckdb
import re
from pathlib import Path
from configparser import ConfigParser
from typing import Dict, List, Tuple


def clean_sql_query(query: str, context: str = "") -> tuple[str, bool]:
    """
    Nettoie et valide une requ√™te SQL extraite.

    Returns:
        (cleaned_query, is_valid)
    """
    # Supprimer le code Python apr√®s la requ√™te
    # Chercher des patterns Python communs
    python_patterns = [
        r'\)\s*#',  # Commentaire Python
        r'\)\s*def\s+',  # D√©finition de fonction
        r'\)\s*return\s+',  # Return statement
        r'\)\s*with\s+',  # With statement
        r'\)\s*if\s+',  # If statement
        r'"\s*if\s+',  # Fin de string avec if
    ]

    for pattern in python_patterns:
        match = re.search(pattern, query)
        if match:
            query = query[:match.start()].strip()
            break

    # V√©rifier si la requ√™te contient des templates non r√©solus
    has_templates = bool(re.search(r'\{[^}]+\}', query))

    # V√©rifier si c'est une vraie requ√™te SQL compl√®te
    has_select = 'SELECT' in query.upper()
    has_from = 'FROM' in query.upper()
    is_complete = has_select and has_from and not has_templates

    # Nettoyer les espaces multiples
    query = re.sub(r'\s+', ' ', query).strip()

    return query, is_complete


def extract_sql_queries_from_code() -> Dict[str, List[Dict]]:
    """
    Scanne tous les fichiers .py et .ipynb pour trouver les requ√™tes SQL.

    Returns:
        Dict avec {fichier_source: [{"query": sql, "line": num, "context": str, "is_complete": bool}]}
    """
    project_root = Path(__file__).parent.parent
    sql_queries = {}

    # Patterns am√©lior√©s pour d√©tecter UNIQUEMENT les requ√™tes SQL compl√®tes
    patterns = [
        # Requ√™tes SQL dans conn.execute() avec triple quotes (plus pr√©cis)
        (r'conn\.execute\(\s*f?"""[\s]*(SELECT[\s\S]+?FROM[\s\S]+?)[\s]*"""\s*\)', "conn.execute triple quotes"),
        (r'conn\.execute\(\s*f?\'\'\'[\s]*(SELECT[\s\S]+?FROM[\s\S]+?)[\s]*\'\'\'\s*\)', "conn.execute triple quotes"),

        # Variables SQL avec triple quotes
        (r'(?:sql|query|QUERY)\s*=\s*f?"""[\s]*(SELECT[\s\S]+?FROM[\s\S]+?)[\s]*"""', "variable triple quotes"),
        (r'(?:sql|query|QUERY)\s*=\s*f?\'\'\'[\s]*(SELECT[\s\S]+?FROM[\s\S]+?)[\s]*\'\'\'', "variable triple quotes"),

        # Requ√™tes SQL sur une seule ligne
        (r'conn\.execute\(\s*f?["\']+(SELECT\s+.+?\s+FROM\s+.+?)["\']', "conn.execute single line"),
        (r'(?:sql|query|QUERY)\s*=\s*f?["\']+(SELECT\s+.+?\s+FROM\s+.+?)["\']', "variable single line"),
    ]

    # R√©pertoires √† scanner
    scan_dirs = ['00_eda', '10_preprod', '20_prod']

    for scan_dir in scan_dirs:
        dir_path = project_root / scan_dir
        if not dir_path.exists():
            continue

        # Scanner les fichiers Python
        for py_file in dir_path.rglob("*.py"):
            # Ignorer venv et pycache
            if '.venv' in str(py_file) or '__pycache__' in str(py_file) or 'site-packages' in str(py_file):
                continue

            try:
                content = py_file.read_text(encoding='utf-8')
                lines = content.split('\n')
                relative_path = str(py_file.relative_to(project_root))

                for pattern, pattern_type in patterns:
                    matches = re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE)

                    for match in matches:
                        raw_query = match.group(1).strip()

                        # Trouver le num√©ro de ligne
                        line_num = content[:match.start()].count('\n') + 1

                        # Contexte: quelques lignes avant
                        context_start = max(0, line_num - 3)
                        context = '\n'.join(lines[context_start:line_num])

                        # Nettoyer et valider la requ√™te
                        cleaned_query, is_complete = clean_sql_query(raw_query, context)

                        # Ignorer les exemples/tests g√©n√©riques
                        if ('iris' in cleaned_query.lower() or
                            'example' in cleaned_query.lower() or
                            ('test' in cleaned_query.lower() and 'interactions_test' not in cleaned_query.lower())):
                            continue

                        # Ignorer les requ√™tes trop courtes (probablement mal extraites)
                        if len(cleaned_query) < 20:
                            continue

                        if relative_path not in sql_queries:
                            sql_queries[relative_path] = []

                        sql_queries[relative_path].append({
                            'query': cleaned_query,
                            'line': line_num,
                            'type': pattern_type,
                            'context': context[-150:] if len(context) > 150 else context,
                            'is_complete': is_complete,
                            'raw_query': raw_query[:100]  # Pour debug
                        })

            except Exception as e:
                print(f"Warning: Could not read {py_file}: {e}")

        # Scanner les notebooks Jupyter
        for nb_file in dir_path.rglob("*.ipynb"):
            if '.venv' in str(nb_file) or '__pycache__' in str(nb_file):
                continue

            try:
                content = nb_file.read_text(encoding='utf-8')
                relative_path = str(nb_file.relative_to(project_root))

                # Pour les notebooks, on cherche dans le JSON
                for pattern, pattern_type in patterns:
                    matches = re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE)

                    for match in matches:
                        raw_query = match.group(1).strip()
                        cleaned_query, is_complete = clean_sql_query(raw_query)

                        if ('iris' in cleaned_query.lower() or
                            'example' in cleaned_query.lower() or
                            ('test' in cleaned_query.lower() and 'interactions_test' not in cleaned_query.lower())):
                            continue

                        if len(cleaned_query) < 20:
                            continue

                        if relative_path not in sql_queries:
                            sql_queries[relative_path] = []

                        sql_queries[relative_path].append({
                            'query': cleaned_query,
                            'line': -1,  # Difficile de d√©terminer dans un notebook
                            'type': f"notebook {pattern_type}",
                            'context': '',
                            'is_complete': is_complete,
                            'raw_query': raw_query[:100]
                        })

            except Exception as e:
                print(f"Warning: Could not read {nb_file}: {e}")

    return sql_queries


# Collecter les requ√™tes SQL au moment de l'import
SQL_QUERIES_MAP = extract_sql_queries_from_code()

# Cr√©er une liste plate pour pytest.parametrize - SEULEMENT les requ√™tes compl√®tes
SQL_QUERIES_LIST = []
SQL_QUERIES_ALL = []  # Toutes les requ√™tes (pour statistiques)

for source_file, queries in SQL_QUERIES_MAP.items():
    for idx, query_info in enumerate(queries):
        query_dict = {
            'id': f"{source_file}:{query_info['line']}:query{idx}",
            'source': source_file,
            'line': query_info['line'],
            'query': query_info['query'],
            'type': query_info['type'],
            'is_complete': query_info.get('is_complete', False)
        }

        SQL_QUERIES_ALL.append(query_dict)

        # Ajouter uniquement les requ√™tes compl√®tes pour les tests
        if query_info.get('is_complete', False):
            SQL_QUERIES_LIST.append(query_dict)

if not SQL_QUERIES_LIST:
    print("‚ö†Ô∏è  Aucune requ√™te SQL trouv√©e dans le code")


@pytest.fixture(scope="module")
def s3_config():
    """Charge la configuration S3 depuis les credentials"""
    credentials_path = Path(__file__).parent.parent / "96_keys" / "credentials"

    if not credentials_path.exists():
        pytest.skip(f"Fichier credentials non trouv√©: {credentials_path}")

    config = ConfigParser()
    config.read(credentials_path)

    if 's3fast' not in config:
        pytest.skip("Section [s3fast] manquante dans credentials")

    return {
        'access_key': config['s3fast']['aws_access_key_id'],
        'secret_key': config['s3fast']['aws_secret_access_key'],
        'endpoint': config['s3fast']['endpoint_url'].replace('http://', ''),
        'region': config['s3fast']['region']
    }


@pytest.fixture(scope="module")
def duckdb_connection(s3_config):
    """Cr√©e une connexion DuckDB configur√©e pour S3 et la base locale"""
    # Connexion √† la base locale si elle existe
    db_path = Path(__file__).parent.parent / "10_preprod" / "data" / "mangetamain.duckdb"

    if db_path.exists():
        conn = duckdb.connect(str(db_path), read_only=True)
    else:
        conn = duckdb.connect()

    # Charger httpfs pour S3
    try:
        conn.execute("LOAD httpfs")
    except:
        conn.execute("INSTALL httpfs")
        conn.execute("LOAD httpfs")

    # Cr√©er le secret S3
    conn.execute(f"""
        CREATE OR REPLACE SECRET garage_s3 (
            TYPE s3,
            KEY_ID '{s3_config['access_key']}',
            SECRET '{s3_config['secret_key']}',
            ENDPOINT '{s3_config['endpoint']}',
            REGION '{s3_config['region']}',
            URL_STYLE 'path',
            USE_SSL false
        )
    """)

    yield conn

    conn.close()


def test_sql_queries_discovered():
    """V√©rifie qu'au moins une requ√™te SQL a √©t√© trouv√©e"""
    total_all = len(SQL_QUERIES_ALL)
    total_complete = len(SQL_QUERIES_LIST)
    total_incomplete = total_all - total_complete

    print(f"\nüìã Requ√™tes SQL d√©couvertes:")
    print(f"   ‚Ä¢ Total: {total_all}")
    print(f"   ‚Ä¢ Compl√®tes (testables): {total_complete}")
    print(f"   ‚Ä¢ Incompl√®tes (templates): {total_incomplete}")

    # Grouper par fichier source (seulement compl√®tes)
    by_source = {}
    for q in SQL_QUERIES_LIST:
        source = q['source']
        if source not in by_source:
            by_source[source] = []
        by_source[source].append(q)

    print(f"\nüìÑ Requ√™tes testables par fichier:")
    for source, queries in sorted(by_source.items()):
        print(f"\n   {source}")
        for q in queries:
            line_info = f"ligne {q['line']}" if q['line'] > 0 else "notebook"
            query_preview = q['query'][:80] + "..." if len(q['query']) > 80 else q['query']
            print(f"      ‚îî‚îÄ {line_info}: {query_preview}")

    assert len(SQL_QUERIES_LIST) > 0, "Aucune requ√™te SQL testable trouv√©e dans le code"


@pytest.mark.parametrize("query_info", SQL_QUERIES_LIST, ids=lambda q: q['id'])
def test_sql_query_valid_syntax(duckdb_connection, query_info):
    """V√©rifie que la syntaxe SQL est valide (EXPLAIN)"""
    query = query_info['query']
    source = query_info['source']
    line = query_info['line']

    try:
        # Utiliser EXPLAIN pour v√©rifier la syntaxe sans ex√©cuter
        duckdb_connection.execute(f"EXPLAIN {query}")

        print(f"\n‚úÖ Syntaxe SQL valide")
        print(f"   Source: {source}:{line}")
        print(f"   Query: {query[:100]}...")

    except Exception as e:
        error_msg = str(e)[:200]
        pytest.fail(f"‚ùå Syntaxe SQL invalide dans {source}:{line}\n   Query: {query}\n   Erreur: {error_msg}")


@pytest.mark.parametrize("query_info", SQL_QUERIES_LIST, ids=lambda q: q['id'])
def test_sql_query_executable(duckdb_connection, query_info):
    """V√©rifie que la requ√™te SQL peut s'ex√©cuter (avec LIMIT 1)"""
    query = query_info['query']
    source = query_info['source']
    line = query_info['line']

    # Ajouter LIMIT 1 si pas d√©j√† pr√©sent pour acc√©l√©rer les tests
    if 'LIMIT' not in query.upper():
        # LIMIT doit √™tre APR√àS ORDER BY
        test_query = f"{query} LIMIT 1"
    else:
        test_query = query

    try:
        result = duckdb_connection.execute(test_query).fetchall()

        print(f"\n‚úÖ Requ√™te ex√©cut√©e avec succ√®s")
        print(f"   Source: {source}:{line}")
        print(f"   R√©sultat: {len(result)} ligne(s)")

    except Exception as e:
        error_msg = str(e)[:300]
        pytest.fail(f"‚ùå Erreur d'ex√©cution dans {source}:{line}\n   Query: {query}\n   Erreur: {error_msg}")


def test_sql_queries_summary(duckdb_connection):
    """R√©sum√© de toutes les requ√™tes SQL test√©es"""
    print("\n" + "="*70)
    print("üìä R√âSUM√â DES REQU√äTES SQL")
    print("="*70)

    # Grouper par fichier
    by_source = {}
    for q in SQL_QUERIES_LIST:
        source = q['source']
        if source not in by_source:
            by_source[source] = []
        by_source[source].append(q)

    total_queries = len(SQL_QUERIES_LIST)
    total_files = len(by_source)

    print(f"\nüìà Statistiques:")
    print(f"   ‚Ä¢ Total de requ√™tes: {total_queries}")
    print(f"   ‚Ä¢ Fichiers avec SQL: {total_files}")
    print(f"   ‚Ä¢ Moyenne par fichier: {total_queries/total_files:.1f}")

    print(f"\nüìÑ D√©tail par fichier:")
    for source, queries in sorted(by_source.items()):
        print(f"   ‚Ä¢ {source}: {len(queries)} requ√™te(s)")

    # Analyser les types de requ√™tes
    s3_queries = sum(1 for q in SQL_QUERIES_LIST if 's3://' in q['query'])
    table_queries = sum(1 for q in SQL_QUERIES_LIST
                       if 'FROM RAW_' in q['query'] or 'FROM PP_' in q['query'] or 'FROM interactions_' in q['query'])

    print(f"\nüîç Types de requ√™tes:")
    print(f"   ‚Ä¢ Requ√™tes S3: {s3_queries}")
    print(f"   ‚Ä¢ Requ√™tes sur tables DuckDB: {table_queries}")
    print(f"   ‚Ä¢ Autres: {total_queries - s3_queries - table_queries}")

    print("="*70 + "\n")


if __name__ == "__main__":
    # Test manuel: afficher les requ√™tes d√©couvertes
    print("="*70)
    print("üîç SCAN DU PROJET POUR REQU√äTES SQL")
    print("="*70)

    queries_map = extract_sql_queries_from_code()

    if queries_map:
        total = sum(len(qs) for qs in queries_map.values())
        print(f"\n‚úÖ {total} requ√™te(s) SQL trouv√©e(s) dans {len(queries_map)} fichier(s):")

        for source, queries in sorted(queries_map.items()):
            print(f"\n   üìÑ {source}")
            for i, q in enumerate(queries, 1):
                line_info = f"ligne {q['line']}" if q['line'] > 0 else "notebook"
                print(f"      {i}. [{line_info}] Type: {q['type']}")
                query_lines = q['query'][:150].replace('\n', ' ')
                print(f"         {query_lines}...")
    else:
        print("\n‚ö†Ô∏è  Aucune requ√™te SQL trouv√©e dans le code")

    print("\n" + "="*70)
    print("Pour tester les requ√™tes SQL, ex√©cuter:")
    print("   pytest test_sql_queries.py -v -s")
    print("="*70 + "\n")
