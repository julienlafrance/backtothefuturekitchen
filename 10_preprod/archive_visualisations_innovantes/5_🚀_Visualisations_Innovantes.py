"""Page Streamlit pour visualisations innovantes.

Cette page pr√©sente des visualisations spectaculaires utilisant Altair et Plotly avanc√©
pour offrir des insights uniques et une exp√©rience "WHAOH".
"""

import streamlit as st
import pandas as pd
import duckdb
from configparser import ConfigParser
from pathlib import Path

from mangetamain_analytics.data.cached_loaders import get_recipes_clean
from mangetamain_analytics.visualization.innovative_charts import (
    create_linked_brushing_dashboard,
    create_calendar_heatmap,
    create_sunburst_hierarchy,
    create_ridgeline_plot,
    create_stream_graph,
    create_parallel_coordinates,
    create_radar_chart_comparison,
)

st.set_page_config(
    page_title="Visualisations Innovantes",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded",
)


@st.cache_data(ttl=3600)
def load_recipes_with_ratings():
    """Charge recettes avec ratings agr√©g√©s par recette via DuckDB."""
    # Charger recettes
    recipes = get_recipes_clean()
    if hasattr(recipes, "to_pandas"):
        recipes = recipes.to_pandas()

    # Charger les credentials S3
    try:
        config = ConfigParser()
        cred_file = '/home/julien/code/mangetamain/000_dev/96_keys/credentials'
        config.read(cred_file)

        creds = {
            'aws_access_key_id': config['s3fast']['aws_access_key_id'],
            'aws_secret_access_key': config['s3fast']['aws_secret_access_key'],
            'endpoint_url': config['s3fast']['endpoint_url'],
            'region_name': config['s3fast']['region'],
            'bucket': config['s3fast']['bucket']
        }
    except Exception as e:
        st.error(f"Impossible de charger les credentials S3: {e}")
        # Retourner les recettes sans ratings
        df = recipes.copy()
        df["rating"] = 3.0
        df["n_ratings"] = 0
        df["n_users"] = 0
        return df

    # Connexion DuckDB pour lire les interactions depuis S3
    conn = duckdb.connect()
    conn.execute("INSTALL httpfs")
    conn.execute("LOAD httpfs")

    # Cr√©er le secret S3
    conn.execute(f"""
        CREATE SECRET s3_secret (
            TYPE S3,
            KEY_ID '{creds['aws_access_key_id']}',
            SECRET '{creds['aws_secret_access_key']}',
            ENDPOINT '{creds['endpoint_url'].replace('http://', '')}',
            REGION '{creds['region_name']}',
            URL_STYLE 'path',
            USE_SSL false
        )
    """)

    # Charger et agr√©ger les ratings depuis S3
    try:
        ratings_agg = conn.execute("""
            SELECT
                recipe_id as id,
                AVG(rating) as rating,
                COUNT(*) as n_ratings,
                COUNT(DISTINCT user_id) as n_users
            FROM 's3://mangetamain/final_interactions.parquet'
            GROUP BY recipe_id
        """).fetchdf()
    except Exception as e:
        st.warning(f"Impossible de charger les ratings: {e}")
        ratings_agg = pd.DataFrame(columns=['id', 'rating', 'n_ratings', 'n_users'])
    finally:
        conn.close()

    # Jointure
    df = recipes.merge(ratings_agg, on="id", how="left")

    # Remplir NaN pour recettes sans ratings
    df["rating"] = df["rating"].fillna(3.0)
    df["n_ratings"] = df["n_ratings"].fillna(0).astype(int)
    df["n_users"] = df["n_users"].fillna(0).astype(int)

    # Convertir la colonne tags en string pour √©viter les probl√®mes de hashing
    if "tags" in df.columns:
        df["tags"] = df["tags"].astype(str)

    # Ajouter colonnes season si manquante
    if "season" not in df.columns and "submitted" in df.columns:
        df["month"] = pd.to_datetime(df["submitted"]).dt.month
        df["season"] = df["month"].map(
            {
                12: "Hiver",
                1: "Hiver",
                2: "Hiver",
                3: "Printemps",
                4: "Printemps",
                5: "Printemps",
                6: "√ât√©",
                7: "√ât√©",
                8: "√ât√©",
                9: "Automne",
                10: "Automne",
                11: "Automne",
            }
        )

    # Ajouter complexity_category si manquante
    if "complexity_category" not in df.columns and "n_steps" in df.columns:
        df["complexity_category"] = pd.cut(
            df["n_steps"], bins=[0, 5, 10, 100], labels=["Facile", "Moyen", "Complexe"]
        )

    return df


@st.cache_data(ttl=3600)
def load_and_prepare_data():
    """Charge et pr√©pare les donn√©es pour visualisations innovantes."""
    df = get_recipes_clean()

    # Convertir Polars vers pandas pour compatibilit√© avec Altair/Plotly
    if hasattr(df, "to_pandas"):
        df = df.to_pandas()

    # Ajouter colonnes calcul√©es si manquantes
    if "season" not in df.columns and "submitted" in df.columns:
        df["month"] = pd.to_datetime(df["submitted"]).dt.month
        df["season"] = df["month"].map(
            {
                12: "Hiver",
                1: "Hiver",
                2: "Hiver",
                3: "Printemps",
                4: "Printemps",
                5: "Printemps",
                6: "√ât√©",
                7: "√ât√©",
                8: "√ât√©",
                9: "Automne",
                10: "Automne",
                11: "Automne",
            }
        )

    if "complexity_category" not in df.columns:
        if "n_steps" in df.columns:
            df["complexity_category"] = pd.cut(
                df["n_steps"],
                bins=[0, 5, 10, 100],
                labels=["Facile", "Moyen", "Complexe"],
            )

    # Calculer note moyenne si n√©cessaire
    if "rating" not in df.columns and "avg_rating" in df.columns:
        df["rating"] = df["avg_rating"]

    return df


@st.cache_data(ttl=3600)
def prepare_temporal_data(df):
    """Pr√©pare donn√©es temporelles pour stream graph."""
    if "submitted" not in df.columns:
        return pd.DataFrame()

    df = df.copy()
    df["date"] = pd.to_datetime(df["submitted"])
    df["year_month"] = df["date"].dt.to_period("M").dt.to_timestamp()

    # Cat√©goriser recettes
    if "tags" in df.columns:
        # Extraire cat√©gorie dominante des tags
        def categorize_recipe(tags):
            if tags is None or (isinstance(tags, float) and pd.isna(tags)):
                return "Autre"
            tags_str = str(tags).lower()
            if "dessert" in tags_str:
                return "Desserts"
            elif "main" in tags_str or "dish" in tags_str:
                return "Plats principaux"
            elif "appetizer" in tags_str:
                return "Entr√©es"
            elif "salad" in tags_str:
                return "Salades"
            elif "soup" in tags_str:
                return "Soupes"
            else:
                return "Autre"

        df["category"] = df["tags"].apply(categorize_recipe)
    else:
        df["category"] = "Recettes"

    # Agr√©ger par mois et cat√©gorie
    temporal = df.groupby(["year_month", "category"]).size().reset_index(name="count")
    temporal.columns = ["date", "category", "count"]

    return temporal


@st.cache_data(ttl=3600)
def prepare_seasonal_profiles(df):
    """Pr√©pare profils saisonniers pour radar chart."""
    if "season" not in df.columns:
        return pd.DataFrame()

    # Filtrer les saisons valides
    df_with_season = df[df["season"].notna()].copy()
    if len(df_with_season) == 0:
        return pd.DataFrame()

    # Calculer max volumes pour normalisation
    season_counts = df_with_season["season"].value_counts()
    max_volume = season_counts.max() if len(season_counts) > 0 else 1

    # Calculer max popularity
    max_popularity = 1
    if "n_ratings" in df_with_season.columns:
        popularity_by_season = df_with_season.groupby("season")["n_ratings"].sum()
        max_popularity = popularity_by_season.max() if len(popularity_by_season) > 0 else 1

    profiles = []
    # Mapping saisons anglais -> fran√ßais pour affichage
    season_mapping = {
        'Spring': 'Printemps',
        'Summer': '√ât√©',
        'Autumn': 'Automne',
        'Winter': 'Hiver'
    }

    for season_en, season_fr in season_mapping.items():
        season_data = df_with_season[df_with_season["season"] == season_en]

        if len(season_data) == 0:
            continue

        # Calculer m√©triques en filtrant les valeurs extr√™mes (outliers)
        volume = len(season_data)

        # Dur√©e: filtrer valeurs < 5 min ou > 180 min (3h)
        if "minutes" in season_data.columns and season_data["minutes"].notna().any():
            duration_filtered = season_data["minutes"][(season_data["minutes"] >= 5) & (season_data["minutes"] <= 180)]
            avg_duration = float(duration_filtered.mean()) if len(duration_filtered) > 0 else 30.0
        else:
            avg_duration = 30.0

        # Complexit√©: filtrer valeurs > 25 √©tapes
        if "n_steps" in season_data.columns and season_data["n_steps"].notna().any():
            steps_filtered = season_data["n_steps"][season_data["n_steps"] <= 25]
            avg_complexity = float(steps_filtered.mean()) if len(steps_filtered) > 0 else 7.0
        else:
            avg_complexity = 7.0

        # Ingr√©dients: filtrer valeurs < 2 ou > 25
        if "n_ingredients" in season_data.columns and season_data["n_ingredients"].notna().any():
            ingredients_filtered = season_data["n_ingredients"][(season_data["n_ingredients"] >= 2) & (season_data["n_ingredients"] <= 25)]
            avg_ingredients = float(ingredients_filtered.mean()) if len(ingredients_filtered) > 0 else 9.0
        else:
            avg_ingredients = 9.0

        avg_rating = float(season_data["rating"].mean()) if "rating" in season_data.columns and season_data["rating"].notna().any() else 4.0

        popularity = volume
        if "n_ratings" in season_data.columns:
            popularity = float(season_data["n_ratings"].sum())

        # Normaliser sur [0, 100] - ajuster √©chelles pour mieux couvrir le radar
        # Utiliser des valeurs max plus r√©alistes bas√©es sur les moyennes attendues
        profiles.append(
            {
                "season": season_fr,
                "volume_norm": min(100, (volume / max_volume) * 100) if max_volume > 0 else 50,
                "duration_norm": min(100, (avg_duration / 60) * 100),  # Max 60 min au lieu de 120
                "complexity_norm": min(100, (avg_complexity / 12) * 100),  # Max 12 √©tapes au lieu de 20
                "ingredients_norm": min(100, (avg_ingredients / 12) * 100),  # Max 12 ingr√©dients au lieu de 20
                "rating_norm": (avg_rating / 5) * 100,
                "popularity_norm": min(100, (popularity / max_popularity) * 100) if max_popularity > 0 else 50,
            }
        )

    return pd.DataFrame(profiles)


def main():
    """Interface principale de la page."""
    st.title("üöÄ Visualisations Innovantes")

    st.markdown(
        """
    Explorez les donn√©es avec des visualisations spectaculaires utilisant **Altair** et **Plotly avanc√©**.
    Ces graphiques offrent des interactions et insights impossibles avec des visualisations standard.
    """
    )

    # Chargement donn√©es
    with st.spinner("Chargement des donn√©es..."):
        df = load_recipes_with_ratings()

    # Filtres globaux dans sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Filtres")

        # Filtre ann√©es
        if "submitted" in df.columns:
            years = sorted(pd.to_datetime(df["submitted"]).dt.year.unique())
            year_range = st.slider(
                "Plage d'ann√©es",
                min_value=int(min(years)),
                max_value=int(max(years)),
                value=(int(min(years)), int(max(years))),
            )

            df = df[
                (pd.to_datetime(df["submitted"]).dt.year >= year_range[0])
                & (pd.to_datetime(df["submitted"]).dt.year <= year_range[1])
            ]

        # √âchantillonage pour performance
        sample_size = st.number_input(
            "Taille √©chantillon (pour performance)",
            min_value=1000,
            max_value=len(df),
            value=min(10000, len(df)),
            step=1000,
            help="R√©duire pour am√©liorer performance sur graphiques lourds",
        )

        df_sample = df.sample(n=min(sample_size, len(df)), random_state=42)

        st.metric("üìä Recettes affich√©es", f"{len(df_sample):,}")

    # Tabs pour organiser visualisations
    tab1, tab2, tab3, tab4 = st.tabs(
        [
            "üîó Interactif Altair",
            "üìä Plotly Avanc√©",
            "üåä Flux Temporels",
            "üéØ Multi-Dimensions",
        ]
    )

    with tab1:
        st.header("üîó Visualisations Interactives Altair")

        st.subheader("üìç Linked Brushing Dashboard")
        st.markdown(
            """
        **Cliquez-glissez** sur le graphique du haut pour filtrer automatiquement l'histogramme.
        D√©couvrez les corr√©lations entre dur√©e, note et nombre d'√©tapes !
        """
        )

        if all(
            col in df_sample.columns
            for col in ["minutes", "rating", "n_steps", "season"]
        ):
            # Nettoyer donn√©es
            df_viz = df_sample[
                ["minutes", "rating", "n_steps", "season", "name"]
            ].copy()
            df_viz = df_viz.dropna()
            df_viz = df_viz[
                (df_viz["minutes"] > 0)
                & (df_viz["minutes"] < 300)
                & (df_viz["rating"] >= 1)
                & (df_viz["rating"] <= 5)
            ]

            chart = create_linked_brushing_dashboard(df_viz)
            st.altair_chart(chart, use_container_width=True)
        else:
            st.warning("Colonnes n√©cessaires manquantes pour linked brushing")

        st.divider()

        st.subheader("üèîÔ∏è Ridgeline Plot - Distribution des Notes")
        st.markdown(
            "Style **Joy Division** montrant l'√©volution de la distribution des notes ann√©e par ann√©e."
        )

        if "rating" in df_sample.columns and "submitted" in df.columns:
            df_ridge = df_sample[["rating", "submitted"]].copy()
            df_ridge["year"] = pd.to_datetime(df_ridge["submitted"]).dt.year
            df_ridge = df_ridge.dropna()

            # Limiter aux ann√©es avec assez de donn√©es
            year_counts = df_ridge["year"].value_counts()
            valid_years = year_counts[year_counts > 100].index
            df_ridge = df_ridge[df_ridge["year"].isin(valid_years)]

            if len(df_ridge) > 0:
                chart = create_ridgeline_plot(df_ridge)
                st.altair_chart(chart, use_container_width=True)
            else:
                st.info("Pas assez de donn√©es pour le ridgeline plot")
        else:
            st.warning("Colonnes rating et submitted n√©cessaires")

    with tab2:
        st.header("üìä Plotly Avanc√©")

        st.subheader("üìÖ Calendar Heatmap - Activit√© Quotidienne")
        st.markdown(
            "Visualisation style **GitHub contributions** de l'activit√© par jour."
        )

        if "submitted" in df.columns:
            year_to_viz = st.selectbox(
                "S√©lectionner ann√©e",
                options=sorted(
                    pd.to_datetime(df["submitted"]).dt.year.unique(), reverse=True
                ),
            )

            df_cal = df[pd.to_datetime(df["submitted"]).dt.year == year_to_viz]

            if len(df_cal) > 0:
                fig = create_calendar_heatmap(df_cal, year=year_to_viz)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info(f"Pas de donn√©es pour {year_to_viz}")
        else:
            st.warning("Colonne submitted n√©cessaire")

        st.divider()

        st.subheader("üåª Sunburst - Hi√©rarchie Saisons ‚Üí Complexit√©")
        st.markdown("**Cliquez** sur une section pour zoomer dans la hi√©rarchie.")

        if all(col in df_sample.columns for col in ["season", "complexity_category"]):
            df_sun = df_sample[["season", "complexity_category"]].copy()
            df_sun = df_sun.dropna()

            if len(df_sun) > 0:
                fig = create_sunburst_hierarchy(df_sun)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("Pas assez de donn√©es pour sunburst")
        else:
            st.warning("Colonnes season et complexity_category n√©cessaires")

    with tab3:
        st.header("üåä Flux Temporels")

        st.subheader("üåä Stream Graph - √âvolution des Cat√©gories")
        st.markdown(
            "Flux temporel montrant comment les cat√©gories de recettes √©voluent."
        )

        temporal_data = prepare_temporal_data(df)

        if len(temporal_data) > 0:
            fig = create_stream_graph(temporal_data)
            st.plotly_chart(fig, use_container_width=True)

            # Stats
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric(
                    "üìÜ P√©riode couverte",
                    f"{temporal_data['date'].min().strftime('%Y')} - "
                    f"{temporal_data['date'].max().strftime('%Y')}",
                )
            with col2:
                st.metric("üè∑Ô∏è Cat√©gories", len(temporal_data["category"].unique()))
            with col3:
                st.metric("üìä Points temporels", len(temporal_data["date"].unique()))
        else:
            st.warning(
                "Impossible de cr√©er le stream graph avec les donn√©es disponibles"
            )

    with tab4:
        st.header("üéØ Exploration Multi-Dimensions")

        st.subheader("üéØ Parallel Coordinates - Filtrage Multi-Crit√®res")
        st.markdown(
            """
        **Glissez** les barres verticales pour filtrer interactivement sur plusieurs dimensions.
        Trouvez les recettes qui matchent exactement vos crit√®res !
        """
        )

        if all(col in df_sample.columns for col in ["minutes", "n_steps", "rating"]):
            df_par = df_sample[
                ["minutes", "n_steps", "n_ingredients", "rating", "season"]
            ].copy()
            df_par = df_par.dropna()

            if len(df_par) > 5000:
                st.info(
                    f"üìä √âchantillon de 5000 recettes pour performance (total: {len(df_par):,})"
                )
                df_par = df_par.sample(n=5000, random_state=42)

            if len(df_par) > 0:
                fig = create_parallel_coordinates(df_par)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("Pas assez de donn√©es pour parallel coordinates")
        else:
            st.warning("Colonnes num√©riques n√©cessaires manquantes")

        st.divider()

        st.subheader("üìä Radar Chart - Profils Saisonniers")
        st.markdown("Comparaison visuelle des profils de recettes par saison.")

        profiles = prepare_seasonal_profiles(df)

        if len(profiles) > 0:
            fig = create_radar_chart_comparison(profiles)
            st.plotly_chart(fig, use_container_width=True)

            # Explication
            with st.expander("‚ÑπÔ∏è Interpr√©tation du radar chart"):
                st.markdown(
                    """
                - **Volume**: Nombre relatif de recettes
                - **Dur√©e moy.**: Temps de pr√©paration moyen
                - **Complexit√©**: Nombre moyen d'√©tapes
                - **Ingr√©dients**: Nombre moyen d'ingr√©dients
                - **Note**: Note moyenne normalis√©e
                - **Popularit√©**: Volume d'interactions

                Plus la zone est grande, plus la saison est marqu√©e sur cette dimension.
                """
                )
        else:
            st.warning("Impossible de cr√©er les profils saisonniers")

    # Footer
    st.divider()
    st.caption(
        """
    üöÄ Visualisations cr√©√©es avec **Altair** (grammaire Vega-Lite) et **Plotly**
    pour une exp√©rience interactive maximale.
    """
    )


if __name__ == "__main__":
    main()
