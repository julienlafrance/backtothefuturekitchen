"""Streamlit application for Mangetamain Analytics - Updated Version.

This module provides the main interface for analyzing Food.com dataset
using DuckDB as the backend database with all imported tables.
"""

import streamlit as st
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import duckdb
from pathlib import Path
from loguru import logger
import sys
import os
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime

def detect_environment():
    """Detect if running in PREPROD or PROD environment."""
    current_path = str(Path.cwd())
    
    if Path("/.dockerenv").exists():
        return "PROD (Docker)"
    
    if "00_preprod" in current_path:
        return "PREPROD"
    elif "10_prod" in current_path:
        return "PROD"
    else:
        return "UNKNOWN"

def display_environment_badge():
    """Display environment badge in sidebar."""
    env = detect_environment()
    
    if "PREPROD" in env:
        st.sidebar.markdown(
            """
            <div style="background-color: #6c757d; padding: 6px; border-radius: 5px; text-align: center; margin-top: 15px;">
                <small style="color: white; margin: 0; font-weight: bold;">üîß PREPROD</small>
                <p style="color: white; margin: 0; font-size: 9px;">Environnement de d√©veloppement</p>
            </div>
            """, 
            unsafe_allow_html=True
        )
    elif "PROD" in env:
        st.sidebar.markdown(
            """
            <div style="background-color: #28a745; padding: 6px; border-radius: 5px; text-align: center; margin-top: 15px;">
                <small style="color: white; margin: 0; font-weight: bold;">üöÄ PRODUCTION</small>
                <p style="color: white; margin: 0; font-size: 9px;">Environnement de production</p>
            </div>
            """, 
            unsafe_allow_html=True
        )

# Ensure logs directory exists
Path("logs").mkdir(exist_ok=True)

# Configure Loguru logger (only once)
if not any("logs/mangetamain" in str(handler) for handler in logger._core.handlers.values()):
    logger.remove()
    
    logger.add("logs/mangetamain_app.log", 
              rotation="1 MB", 
              level="INFO",
              format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name} | {message}")

    logger.add("logs/mangetamain_errors.log", 
              rotation="1 MB", 
              level="ERROR",
              format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name} | {message}")

    logger.add(sys.stderr, level="DEBUG")

# Page configuration
st.set_page_config(
    page_title="Mangetamain Analytics",
    page_icon="üçΩÔ∏è",
    layout="wide"
)

def get_db_connection():
    """Establish connection to the DuckDB database."""
    db_path = "data/mangetamain.duckdb"
    
    if Path(db_path).exists():
        try:
            conn = duckdb.connect(db_path)
            
            file_size = Path(db_path).stat().st_size / (1024 * 1024)
            tables = conn.execute("SHOW TABLES").fetchall()
            
            logger.info(f"‚úÖ DuckDB connection established - File: {db_path}")
            logger.info(f"üìä Database size: {file_size:.1f} MB")
            logger.info(f"üóÇÔ∏è Tables found: {len(tables)} - {[t[0] for t in tables]}")
            
            return conn
            
        except Exception as e:
            logger.error(f"‚ùå Failed to connect to DuckDB: {e}")
            return None
    else:
        logger.error(f"‚ùå DuckDB file not found: {db_path}")
        return None

def display_database_info(conn):
    """Display comprehensive database information."""
    st.header("üìä Base de donn√©es")
    
    # Database file info
    db_path = "data/mangetamain.duckdb"
    if Path(db_path).exists():
        file_size = Path(db_path).stat().st_size / (1024 * 1024)
        st.success(f"‚úÖ **Fichier DuckDB connect√©**")
        st.code(f"üìÅ {db_path}")
        st.write(f"üìè Taille: {file_size:.1f} MB")
    else:
        st.error(f"‚ùå Fichier non trouv√©: {db_path}")
        return
    
    st.markdown("---")
    
    # Available tables with detailed stats
    st.subheader("üóÇÔ∏è Tables disponibles")
    tables = conn.execute("SHOW TABLES").fetchall()
    
    total_rows = 0
    for table_name, in tables:
        count = conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
        columns = conn.execute(f"DESCRIBE {table_name}").fetchall()
        total_rows += count
        
        # Color coding by table type
        if table_name.startswith('RAW_'):
            emoji = "üì•"  # Raw data
            color = "#ff9999"
        elif table_name.startswith('PP_'):
            emoji = "‚öôÔ∏è"   # Preprocessed data
            color = "#99ccff"
        elif 'interactions_' in table_name:
            emoji = "üéØ"  # ML datasets
            color = "#99ff99"
        else:
            emoji = "üìä"
            color = "#ffcc99"
            
        st.markdown(f"""
        <div style="background-color: {color}; padding: 8px; border-radius: 5px; margin: 2px 0;">
            <strong>{emoji} {table_name}</strong>: {count:,} lignes, {len(columns)} colonnes
        </div>
        """, unsafe_allow_html=True)
    
    st.write(f"**üìà Total**: {total_rows:,} lignes dans {len(tables)} tables")
    
    st.markdown("---")

def create_tables_overview(conn):
    """Create interactive overview of all tables."""
    st.subheader("üìä Vue d'ensemble des tables")
    
    # Get table statistics
    tables = conn.execute("SHOW TABLES").fetchall()
    table_stats = []
    
    for table_name, in tables:
        count = conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
        columns = conn.execute(f"DESCRIBE {table_name}").fetchall()
        
        # Categorize tables
        if table_name.startswith('RAW_'):
            category = "Donn√©es brutes"
        elif table_name.startswith('PP_'):
            category = "Donn√©es pr√©process√©es"
        elif 'interactions_' in table_name:
            category = "Datasets ML"
        else:
            category = "Autres"
            
        table_stats.append({
            'Table': table_name,
            'Lignes': count,
            'Colonnes': len(columns),
            'Cat√©gorie': category
        })
    
    df_stats = pd.DataFrame(table_stats)
    
    # Interactive bar chart with Plotly
    fig = px.bar(df_stats, 
                 x='Table', 
                 y='Lignes',
                 color='Cat√©gorie',
                 title='Nombre de lignes par table',
                 hover_data=['Colonnes'])
    
    fig.update_layout(xaxis_tickangle=-45)
    st.plotly_chart(fig, use_container_width=True)
    
    # Display as table
    st.dataframe(df_stats, use_container_width=True)

def create_rating_analysis(conn):
    """Enhanced rating distribution analysis."""
    st.subheader("‚≠ê Analyse des notes")
    
    # Try different interaction tables
    rating_tables = ['RAW_interactions', 'interactions_train', 'interactions_test', 'interactions_validation']
    
    for table in rating_tables:
        try:
            # Check if table exists and has rating column
            schema = conn.execute(f"DESCRIBE {table}").fetchall()
            column_names = [col[0] for col in schema]
            
            if 'rating' in column_names:
                st.write(f"üìä Analyse des notes - Table: **{table}**")
                
                ratings_df = conn.execute(f"""
                    SELECT rating, COUNT(*) as count
                    FROM {table}
                    WHERE rating IS NOT NULL
                    GROUP BY rating
                    ORDER BY rating
                """).fetchdf()
                
                if not ratings_df.empty:
                    # Create interactive pie chart
                    fig = px.pie(ratings_df, 
                                values='count', 
                                names='rating',
                                title=f'Distribution des notes - {table}')
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Statistics
                    total_ratings = ratings_df['count'].sum()
                    avg_rating = (ratings_df['rating'] * ratings_df['count']).sum() / total_ratings
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Total √©valuations", f"{total_ratings:,}")
                    with col2:
                        st.metric("Note moyenne", f"{avg_rating:.2f} ‚≠ê")
                    with col3:
                        most_common = ratings_df.loc[ratings_df['count'].idxmax(), 'rating']
                        st.metric("Note la plus fr√©quente", f"{most_common} ‚≠ê")
                    
                    break
                    
        except Exception as e:
            continue
    else:
        st.warning("‚ö†Ô∏è Aucune table avec des notes trouv√©e")

def create_temporal_analysis(conn):
    """Analyze temporal patterns in interactions."""
    st.subheader("üìÖ Analyse temporelle")
    
    # Look for date columns in interaction tables
    tables_with_dates = []
    
    for table in ['RAW_interactions', 'interactions_train', 'interactions_test', 'interactions_validation']:
        try:
            schema = conn.execute(f"DESCRIBE {table}").fetchall()
            column_names = [col[0] for col in schema]
            
            if 'date' in column_names:
                tables_with_dates.append(table)
        except:
            continue
    
    if tables_with_dates:
        selected_table = st.selectbox("Choisir une table:", tables_with_dates)
        
        try:
            # Get temporal data
            temporal_df = conn.execute(f"""
                SELECT date, COUNT(*) as interactions_count
                FROM {selected_table}
                WHERE date IS NOT NULL
                GROUP BY date
                ORDER BY date
                LIMIT 1000
            """).fetchdf()
            
            if not temporal_df.empty:
                # Convert to datetime
                temporal_df['date'] = pd.to_datetime(temporal_df['date'])
                
                # Create time series plot
                fig = px.line(temporal_df, 
                             x='date', 
                             y='interactions_count',
                             title=f'√âvolution des interactions dans le temps - {selected_table}')
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Time range info
                date_range = temporal_df['date'].max() - temporal_df['date'].min()
                st.info(f"üìä P√©riode analys√©e: du {temporal_df['date'].min().strftime('%Y-%m-%d')} "
                       f"au {temporal_df['date'].max().strftime('%Y-%m-%d')} "
                       f"({date_range.days} jours)")
                
        except Exception as e:
            st.error(f"‚ùå Erreur lors de l'analyse temporelle: {e}")
    else:
        st.warning("‚ö†Ô∏è Aucune table avec des dates trouv√©e")

def create_user_analysis(conn):
    """Enhanced user activity analysis."""
    st.subheader("üë• Analyse des utilisateurs")
    
    try:
        # Get user data from PP_users table
        users_df = conn.execute("""
            SELECT n_items, n_ratings
            FROM PP_users 
            WHERE n_ratings > 0 AND n_items > 0
            ORDER BY n_ratings DESC
            LIMIT 10000
        """).fetchdf()
        
        if not users_df.empty:
            # Create scatter plot
            fig = px.scatter(users_df, 
                           x='n_items', 
                           y='n_ratings',
                           title='Relation entre nombre de recettes et nombre d\'√©valuations',
                           labels={'n_items': 'Nombre de recettes', 'n_ratings': 'Nombre d\'√©valuations'},
                           opacity=0.6)
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Statistics
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Utilisateurs actifs", f"{len(users_df):,}")
            with col2:
                st.metric("Recettes moy./utilisateur", f"{users_df['n_items'].mean():.1f}")
            with col3:
                st.metric("√âvaluations moy./utilisateur", f"{users_df['n_ratings'].mean():.1f}")
            with col4:
                correlation = users_df['n_items'].corr(users_df['n_ratings'])
                st.metric("Corr√©lation", f"{correlation:.3f}")
                
        else:
            st.warning("‚ö†Ô∏è Aucune donn√©e utilisateur trouv√©e")
            
    except Exception as e:
        st.error(f"‚ùå Erreur lors de l'analyse des utilisateurs: {e}")

def display_raw_data_explorer(conn):
    """Interactive data explorer."""
    with st.expander("üîç Explorateur de donn√©es"):
        # Table selector
        tables = conn.execute("SHOW TABLES").fetchall()
        table_names = [t[0] for t in tables]
        
        selected_table = st.selectbox("S√©lectionner une table:", table_names)
        
        if selected_table:
            # Get table info
            count = conn.execute(f"SELECT COUNT(*) FROM {selected_table}").fetchone()[0]
            schema = conn.execute(f"DESCRIBE {selected_table}").fetchall()
            
            st.write(f"**{selected_table}**: {count:,} lignes, {len(schema)} colonnes")
            
            # Column info
            st.write("**Colonnes:**")
            for col_name, col_type, _, _, _, _ in schema:
                st.write(f"- `{col_name}` ({col_type})")
            
            # Sample size selector
            sample_size = st.slider("Nombre de lignes √† afficher:", 10, 1000, 100)
            
            # Display sample data
            sample_df = conn.execute(f"SELECT * FROM {selected_table} LIMIT {sample_size}").fetchdf()
            st.dataframe(sample_df, use_container_width=True)

def main():
    """Main Streamlit application - Enhanced version."""
    logger.info("üöÄ Enhanced Streamlit application starting")
    
    st.title("üçΩÔ∏è Mangetamain Analytics - Version Compl√®te")
    st.markdown("*Analyse compl√®te des donn√©es Food.com avec toutes les tables import√©es*")
    
    # Database connection
    conn = get_db_connection()
    if not conn:
        st.error("‚ùå Impossible de se connecter √† la base DuckDB")
        st.info("üí° Assurez-vous que le fichier `data/mangetamain.duckdb` existe")
        return
    
    # Sidebar
    with st.sidebar:
        display_database_info(conn)
        display_environment_badge()
    
    # Main content tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üìä Vue d'ensemble", 
        "‚≠ê Analyses des notes", 
        "üìÖ Analyse temporelle", 
        "üë• Utilisateurs", 
        "üîç Donn√©es brutes"
    ])
    
    with tab1:
        create_tables_overview(conn)
    
    with tab2:
        create_rating_analysis(conn)
    
    with tab3:
        create_temporal_analysis(conn)
    
    with tab4:
        create_user_analysis(conn)
    
    with tab5:
        display_raw_data_explorer(conn)
    
    # Footer
    st.markdown("---")
    st.markdown("*üìä Mangetamain Analytics - Donn√©es Food.com | üîß PREPROD Environment*")
    
    logger.info("‚úÖ Application fully loaded")

if __name__ == "__main__":
    logger.info("üåü Starting Enhanced Mangetamain Analytics")
    main()
